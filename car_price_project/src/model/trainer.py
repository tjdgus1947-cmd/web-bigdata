# src/model/trainer.py

import os
from dataclasses import dataclass

import joblib
import numpy as np
import pandas as pd
from sklearn.compose import ColumnTransformer
from sklearn.ensemble import RandomForestRegressor, VotingRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder
from xgboost import XGBRegressor


@dataclass
class ModelTrainer:
    data_path: str = "data/processed/encar_processed.csv"
    model_path: str = "models/price_model.pkl"
    use_ensemble: bool = True  # ì•™ìƒë¸” ëª¨ë¸ ì‚¬ìš© ì—¬ë¶€
    tune_hyperparameters: bool = False  # í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì—¬ë¶€

    def load(self) -> pd.DataFrame:
        df = pd.read_csv(self.data_path)
        print(f"ğŸ“Š í•™ìŠµ ë°ì´í„° ë¡œë“œ: {df.shape}")
        return df

    def add_feature_engineering(self, df: pd.DataFrame) -> pd.DataFrame:
        """íŒŒìƒ ë³€ìˆ˜ ìƒì„±"""
        print("ğŸ”§ Feature Engineering ì§„í–‰ ì¤‘...")
        
        # 1) ì—°í‰ê·  ì£¼í–‰ê±°ë¦¬
        df['Mileage_per_year'] = df['Mileage'] / (df['CarAge'] + 1)  # 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€
        
        # 2) ê³ ê¸‰ ë¸Œëœë“œ ì—¬ë¶€
        luxury_brands = ['BMW', 'Mercedes-Benz', 'ë©”ë¥´ì„¸ë°ìŠ¤-ë²¤ì¸ ', 'Audi', 'ì•„ìš°ë””', 
                        'Porsche', 'í¬ë¥´ì‰', 'Lexus', 'ë ‰ì„œìŠ¤', 'Genesis', 'ì œë„¤ì‹œìŠ¤']
        df['Is_luxury_brand'] = df['Manufacturer'].isin(luxury_brands).astype(int)
        
        # 3) ì°¨ëŸ‰ ì—°ì‹ ê·¸ë£¹ (ì‹ ì°¨, ì¤€ì‹ ì°¨, ì¤‘ê³ , ë…¸í›„)
        df['Age_group'] = pd.cut(df['CarAge'], 
                                 bins=[-1, 2, 5, 10, 100], 
                                 labels=['ì‹ ì°¨ê¸‰', 'ì¤€ì‹ ì°¨', 'ì¤‘ê³ ', 'ë…¸í›„'])
        
        # 4) ì£¼í–‰ê±°ë¦¬ ê·¸ë£¹
        df['Mileage_group'] = pd.cut(df['Mileage'], 
                                      bins=[0, 30000, 80000, 150000, 1000000],
                                      labels=['ì €ì£¼í–‰', 'ì¤‘ì£¼í–‰', 'ê³ ì£¼í–‰', 'ê³¼ë‹¤ì£¼í–‰'])
        
        print(f"âœ… íŒŒìƒ ë³€ìˆ˜ ì¶”ê°€ ì™„ë£Œ: {df.shape}")
        return df

    def train(self):
        df = self.load()
        df = self.add_feature_engineering(df)

        # -------------------------
        # 1) Feature / Target ë¶„ë¦¬
        # -------------------------
        numeric_features = ["CarAge", "Mileage", "Mileage_per_year", "Is_luxury_brand"]
        categorical_features = [
            "Manufacturer",
            "Model",
            "Badge",
            "FuelType",
            "Transmission",
            "OfficeCityState",
            "Age_group",
            "Mileage_group"
        ]

        feature_cols = numeric_features + categorical_features

        X = df[feature_cols].copy()
        y = df["Price"].astype(float)

        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42
        )

        # -------------------------
        # 2) ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
        # -------------------------
        preprocessor = ColumnTransformer(
            transformers=[
                ("num", "passthrough", numeric_features),
                (
                    "cat",
                    OneHotEncoder(handle_unknown="ignore", sparse_output=False),
                    categorical_features,
                ),
            ]
        )

        # -------------------------
        # 3) ëª¨ë¸ ì •ì˜
        # -------------------------
        if self.use_ensemble:
            print("ğŸš€ ì•™ìƒë¸” ëª¨ë¸(RF + XGBoost) í•™ìŠµ ì¤‘...")
            
            rf = RandomForestRegressor(
                n_estimators=200,
                max_depth=20,
                min_samples_split=5,
                min_samples_leaf=2,
                n_jobs=-1,
                random_state=42,
            )
            
            xgb = XGBRegressor(
                n_estimators=200,
                max_depth=8,
                learning_rate=0.1,
                subsample=0.8,
                colsample_bytree=0.8,
                random_state=42,
                n_jobs=-1
            )
            
            # VotingRegressorë¡œ ì•™ìƒë¸”
            ensemble = VotingRegressor(
                estimators=[('rf', rf), ('xgb', xgb)],
                n_jobs=-1
            )
            
            model = Pipeline(
                steps=[
                    ("preprocess", preprocessor),
                    ("model", ensemble),
                ]
            )
            
        else:
            print("ğŸš€ RandomForest ëª¨ë¸ í•™ìŠµ ì¤‘...")
            rf = RandomForestRegressor(
                n_estimators=300,
                max_depth=None,
                n_jobs=-1,
                random_state=42,
            )
            
            model = Pipeline(
                steps=[
                    ("preprocess", preprocessor),
                    ("model", rf),
                ]
            )

        # -------------------------
        # 4) í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ (ì„ íƒì )
        # -------------------------
        if self.tune_hyperparameters and not self.use_ensemble:
            print("ğŸ” í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì¤‘... (ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
            param_grid = {
                'model__n_estimators': [200, 300, 400],
                'model__max_depth': [15, 20, None],
                'model__min_samples_split': [2, 5, 10],
            }
            
            grid_search = GridSearchCV(
                model, 
                param_grid, 
                cv=3, 
                scoring='neg_mean_absolute_error',
                n_jobs=-1,
                verbose=1
            )
            grid_search.fit(X_train, y_train)
            model = grid_search.best_estimator_
            print(f"âœ… ìµœì  íŒŒë¼ë¯¸í„°: {grid_search.best_params_}")
        else:
            model.fit(X_train, y_train)

        # -------------------------
        # 5) í‰ê°€
        # -------------------------
        y_pred_train = model.predict(X_train)
        y_pred_test = model.predict(X_test)
        
        # Train ì„±ëŠ¥
        train_mae = mean_absolute_error(y_train, y_pred_train)
        train_rmse = mean_squared_error(y_train, y_pred_train, squared=False)
        train_r2 = r2_score(y_train, y_pred_train)
        
        # Test ì„±ëŠ¥
        test_mae = mean_absolute_error(y_test, y_pred_test)
        test_rmse = mean_squared_error(y_test, y_pred_test, squared=False)
        test_r2 = r2_score(y_test, y_pred_test)

        print("\n" + "="*50)
        print(" ëª¨ë¸ í‰ê°€ ê²°ê³¼ (ë‹¨ìœ„: ë§Œì›)")
        print("="*50)
        print(f"\n[Train Set]")
        print(f"   MAE  : {train_mae:,.1f} ë§Œì›")
        print(f"   RMSE : {train_rmse:,.1f} ë§Œì›")
        print(f"   RÂ²   : {train_r2:.4f}")
        
        print(f"\n[Test Set]")
        print(f"   MAE  : {test_mae:,.1f} ë§Œì›")
        print(f"   RMSE : {test_rmse:,.1f} ë§Œì›")
        print(f"   RÂ²   : {test_r2:.4f}")
        
        # ì˜¤ë²„í”¼íŒ… ì²´í¬
        if train_r2 - test_r2 > 0.1:
            print(f"\nâš ï¸  ê³¼ì í•© ì˜ì‹¬: Train RÂ² ({train_r2:.4f}) >> Test RÂ² ({test_r2:.4f})")
        
        print("="*50)

        # -------------------------
        # 6) ê°€ê²©ëŒ€ë³„ ì„±ëŠ¥ ë¶„ì„
        # -------------------------
        self._analyze_by_price_range(y_test, y_pred_test)

        # -------------------------
        # 7) ì €ì¥
        # -------------------------
        os.makedirs(os.path.dirname(self.model_path), exist_ok=True)
        joblib.dump(model, self.model_path)
        print(f"\nğŸ’¾ ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {self.model_path}")
        
        # ë©”íƒ€ë°ì´í„° ì €ì¥
        metadata = {
            "train_mae": train_mae,
            "train_rmse": train_rmse,
            "train_r2": train_r2,
            "test_mae": test_mae,
            "test_rmse": test_rmse,
            "test_r2": test_r2,
            "use_ensemble": self.use_ensemble,
            "n_train": len(X_train),
            "n_test": len(X_test)
        }
        
        metadata_path = self.model_path.replace('.pkl', '_metadata.pkl')
        joblib.dump(metadata, metadata_path)
        print(f"ğŸ’¾ ë©”íƒ€ë°ì´í„° ì €ì¥: {metadata_path}")

        return model, metadata

    def _analyze_by_price_range(self, y_true, y_pred):
        """ê°€ê²©ëŒ€ë³„ MAE ë¶„ì„"""
        print("\n" + "="*50)
        print("ğŸ’° ê°€ê²©ëŒ€ë³„ ì„±ëŠ¥ ë¶„ì„")
        print("="*50)
        
        df_eval = pd.DataFrame({
            'true': y_true,
            'pred': y_pred,
            'error': np.abs(y_true - y_pred)
        })
        
        # ê°€ê²©ëŒ€ êµ¬ê°„ ì„¤ì • (ë§Œì› ë‹¨ìœ„)
        bins = [0, 500, 1000, 2000, 3000, 10000]
        labels = ['~500ë§Œ', '500~1000ë§Œ', '1000~2000ë§Œ', '2000~3000ë§Œ', '3000ë§Œ~']
        
        df_eval['price_range'] = pd.cut(df_eval['true'], bins=bins, labels=labels)
        
        for price_range in labels:
            subset = df_eval[df_eval['price_range'] == price_range]
            if len(subset) > 0:
                mae = subset['error'].mean()
                count = len(subset)
                print(f"  {price_range:15s}: MAE = {mae:>8,.1f} ë§Œì› (n={count:>5,})")


if __name__ == "__main__":
    # ê¸°ë³¸ í•™ìŠµ
    trainer = ModelTrainer(use_ensemble=True, tune_hyperparameters=False)
    trainer.train()